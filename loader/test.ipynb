{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from supabase import create_client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def load_document(filename: str):\n",
    "    \"\"\"\n",
    "    Function use to load a PDF document onto a Supabase vector database.\n",
    "    It will convert it into Markdown, split it by its headers, create an embedding for each chunk.\n",
    "    Finally it will upload each embedded chunk to the 'embeddings' table.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dirname:\n",
    "        # Parse the PDF and convert it to Markdown\n",
    "        os.system(f\"\"\"marker_single \"{filename}\" \"{tmp_dirname}\" --batch_multiplier 1 --ocr_all_pages\"\"\")\n",
    "\n",
    "        # Split the resulting Markdown into chunks\n",
    "        resulting_folder_name = Path(filename).stem\n",
    "        with open(f\"{tmp_dirname}/{resulting_folder_name}/{resulting_folder_name}.md\", \"r\") as f:\n",
    "            doc = f.read()\n",
    "\n",
    "            markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "            ], strip_headers=False)\n",
    "            \n",
    "            chunks = markdown_splitter.split_text(doc)\n",
    "\n",
    "            # Instantiate a Supabase and an OpenAI client\n",
    "            supabase_client = create_client(os.environ.get(\"SUPABASE_URL\"), os.environ.get(\"SUPABASE_KEY\"))\n",
    "            openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "            # Add an incrementing identifier to each chunk\n",
    "            for idx, chunk in enumerate(chunks):\n",
    "                response = openai_client.embeddings.create(\n",
    "                    input=chunk.page_content,\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "\n",
    "                supabase_client.table(\"embeddings\").insert({\n",
    "                    \"name\": f\"{filename}:{idx}\",\n",
    "                    \"content\": chunk.page_content,\n",
    "                    \"embedding\": response.data[0].embedding\n",
    "                }).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device cpu with dtype torch.float32\n",
      "Loaded detection model vikp/surya_layout3 on device cpu with dtype torch.float32\n",
      "Loaded reading order model vikp/surya_order on device cpu with dtype torch.float32\n",
      "Loaded recognition model vikp/surya_rec2 on device cpu with dtype torch.float32\n",
      "Loaded texify model to cpu with torch.float32 dtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n",
      "Recognizing Text: 100%|██████████| 2/2 [00:33<00:00, 16.82s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:04<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved markdown to the /var/folders/b4/t373qrvd4m76swgs_nb9vf9r0000gn/T/tmpaw4n_ez4/Acordo Partilha Assinado folder\n"
     ]
    }
   ],
   "source": [
    "load_document(\"../data/Acordo Partilha Assinado.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
